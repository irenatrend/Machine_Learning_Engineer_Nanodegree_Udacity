{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "The most fundamental question in model building is determining what you would like the model to predict. Since we are trying to predict a discrete binary outcome, whether the student will pass or fail this is classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 30\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "n_students = student_data.shape[0]\n",
    "n_features = student_data.shape[1] -1 # excluding target column\n",
    "n_passed = (student_data[student_data.passed == 'yes']).shape[0]\n",
    "n_failed = (student_data[student_data.passed == 'no']).shape[0]                              \n",
    "grad_rate = (n_passed / float(n_passed + n_failed)) * 100\n",
    "\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, \n",
    "                                                                     y_all, \n",
    "                                                                     test_size=0.24)\n",
    "\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "# Note: If you need a validation set, extract it from within training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CreateTable(dict):\n",
    "    # Overridden dict class which takes a dict in the form {'a': 2, 'b': 3},\n",
    "    # and renders an HTML Table in IPython Notebook.\n",
    "    def _repr_html_(self):\n",
    "        tableId = self['1_trainigTime']['tableId']\n",
    "        \n",
    "        html = [\"<table width=60% id='\" + tableId + \"'>\"]\n",
    "        html.append(\"<tr style='background-color:#E6E6E6; font-weight: bold;'>\")\n",
    "        html.append(\"<td width=25%>{0}</td>\".format(tableId))\n",
    "        html.append(\"<td colspan=3>Training set size </td>\")\n",
    "        html.append(\"</tr>\")\n",
    "        html.append(\"<tr style='background-color:#E6E6E6; font-weight: bold;'>\")\n",
    "        html.append(\"<td width=40%>{0}</td>\".format(''))\n",
    "        html.append(\"<td width=20%>100</td>\".format('100'))\n",
    "        html.append(\"<td width=20%>{0}</td>\".format('200'))\n",
    "        html.append(\"<td width=20%>{0}</td>\".format('300'))\n",
    "        html.append(\"</tr>\")\n",
    "            \n",
    "        for key, value in self.iteritems():\n",
    "            html.append(\"<tr>\")            \n",
    "            html.append(\"<td width=40% style='background-color:#E6E6E6;'>{0}</td>\".format(self[key]['name']))\n",
    "            html.append(\"<td width=20%>{0}</td>\".format(self[key]['100']))\n",
    "            html.append(\"<td width=20%>{0}</td>\".format(self[key]['200']))\n",
    "            html.append(\"<td width=20%>{0}</td>\".format(self[key]['300']))\n",
    "            html.append(\"</tr>\")\n",
    "            \n",
    "        html.append(\"</table>\")\n",
    "                \n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    # print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    # print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "    \n",
    "    model_results['1_trainigTime'][str(len(X_train))] = '{0:.3f}'.format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    # print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    # print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
    "    \n",
    "    model_results['2_predictionTime'][str(len(features))] = '{0:.3f}'.format(end - start)\n",
    "    \n",
    "    return f1_score(target.values, y_pred, pos_label='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    # print \"------------------------------------------\"\n",
    "    # print \"Training set size: {}\".format(len(X_train))\n",
    "    \n",
    "    # Fit model to training data\n",
    "    train_classifier(clf, X_train, y_train)      \n",
    "    \n",
    "    # print \"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train))\n",
    "    # print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))\n",
    "    \n",
    "    train_f1_score = predict_labels(clf, X_train, y_train)    \n",
    "    test_f1_score = predict_labels(clf, X_test, y_test)\n",
    "    \n",
    "    model_results['3_train_f1_score'][str(len(X_train))] = '{0:.3f}'.format(train_f1_score)    \n",
    "    model_results['4_test_f1_score'][str(len(X_train))] = '{0:.3f}'.format(test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the student intervention problem I've deceided to train and evaluate following classifiers:\n",
    "\n",
    "* Decision Tree Classifier\n",
    "* Suport Vector Machines\n",
    "* Randomized Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and predict using different training set sizes and Models\n",
    "train_sizes = [100, 200, 300] # Training Sizes\n",
    "\n",
    "# Model 1: Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Model 2: Suport Vector Machines\n",
    "from sklearn.svm import SVC\n",
    "SVM_clf = SVC()\n",
    "\n",
    "#Model 3: Randomized Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_clf = RandomForestClassifier(n_estimators=15)\n",
    "\n",
    "models = {\"SVM classifier\": SVM_clf, \"Decision Tree Classifier\" : dtc_clf, \"Randomized Forest\": RF_clf}\n",
    "\n",
    "dic_DecisionTrees = {}\n",
    "dic_SVC = {}\n",
    "dic_RandomForest = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    " \n",
    "    model_results = {}\n",
    "    model_results = {'1_trainigTime': {'name': 'Trainig Time'}, \n",
    "        '2_predictionTime': {'name': 'Prediction Time'}, \n",
    "        '3_train_f1_score': {'name': 'F1 score for training set'},\n",
    "        '4_test_f1_score': {'name': 'F1 score for testing set'}}\n",
    "    model_results['1_trainigTime']['tableId'] = model.__class__.__name__\n",
    "\n",
    "    #print \"\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\"\n",
    "    #print \"Testing Model {}\\n\".format(model_name)\n",
    "    for size in train_sizes:\n",
    "        train_predict(model, X_train[:size], y_train[:size], X_test, y_test)\n",
    "        \n",
    "    if (model.__class__.__name__ == \"SVC\"):\n",
    "        dic_SVC = model_results\n",
    "    if (model.__class__.__name__ == \"DecisionTreeClassifier\"):\n",
    "        dic_DecisionTrees = model_results\n",
    "    if (model.__class__.__name__ == \"RandomForestClassifier\"):\n",
    "        dic_RandomForest = model_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Classifier\n",
    "\n",
    "#### What are the general applications of this model\n",
    "\n",
    "A decision tree is the minimum number of yes/no questions that one has to ask to assess the probability of making a correct decision, or we can also say that a decision tree is a set of rules used to classify data into categories. It looks at the variables in a data set, determines which are most important, and then comes up with a tree of decisions which best partitions the data. The tree is created by splitting data up by variables and then counting to see how many are in each bucket after each split.\n",
    "\n",
    "#### Strengths \n",
    "\n",
    "* Simple to understand and to interpret. Trees can be visualized.\n",
    "* If the decision tree is short, it is easy for a human to interpret it.\n",
    "* Able to handle both numerical and categorical data.\n",
    "* Able to handle multi-output problems.\n",
    "* Ability to deal with irrelevant features. The algorithm selects “relevant” features first, and generally ignores irrelevant features.\n",
    "* Requires little data preparation.\n",
    "* Decision trees combined into an ensemble create some of the best binary classifiers.\n",
    "\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* Sometimes decision tree learners can create over complex trees that do not generalize the data well. This is called overfitting and it is one of their main disadvantage. \n",
    "* Decision trees can be unstable because small changes in the data might result in large changes in the tree. This problem is mitigated by using decision trees within an ensemble.\n",
    "* Decision tree learners create biased trees if some classes dominate.\n",
    "* Another disadvantage is that they don’t support online learning, which means that you have to rebuild your tree when new examples come on. \n",
    "\n",
    "\n",
    "#### Why did you choose this model to apply?\n",
    "\n",
    "I thought it would be good potential candidate for modeling the student intervention system because:\n",
    "* Is less complex so should be very time efficient. \n",
    "* A simpler model that is easier to interpret and explain to the audience. It delivers the best humanly understandable results. \n",
    "* Predictive power, relative simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model SVM classifier\n",
    "\n",
    "#### What are the general applications of this model\n",
    "\n",
    "SVM is a non-probabilistic parametric classifier with a broad range of applications. It uses a linear hyperplane for separating the data points, which can also be used as a nonlinear classifier through the use of kernels.\n",
    "\n",
    "* Especially popular in text classification problems where very high-dimensional spaces are the norm.\n",
    "* Hand-written characters can be recognized using SVM.\n",
    "* SVMs are helpful in text and hypertext categorization as their application can significantly reduce the need for labeled training instances.\n",
    "* Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.\n",
    "* SVMs are also useful in medical science to classify proteins with up to 90% of the compounds classified correctly.\n",
    "\n",
    "\n",
    "#### Strengths \n",
    "\n",
    "* SVMs have good generalization performance. High accuracy.\n",
    "* Effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function, so it is also memory efficient.\n",
    "* With an appropriate kernel they can work well even if the data isn't linearly separable in the base feature space.\n",
    "\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* The major downside of SVMs is that they can be painfully inefficient to train. \n",
    "* Expensive training and testing phase, both in speed and size. \n",
    "* It is not recommend for any problem where you have many training examples. \n",
    "* SVMs are not recommend for most \"industry scale\" applications. Anything beyond a toy/lab problem might be better approached with a different algorithm.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation\n",
    "\n",
    "\n",
    "#### Why did you choose this model to apply?\n",
    "\n",
    "* SVM does not require a significant amount of data to make a reasonable prediction. Since the student intervention dataset is small and SVM provides good generalization performance, high accuracy and is less computationally intensive, I thought it would be another good potential candidate for modeling the student intervention system. Also SVM is a powerful and flexible classifier that could be adjusted to specific applications through fine tuning its parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "#### What are the general applications of this model\n",
    "\n",
    "Random Forest Classifier is an ensemble learning method in which a number of relatively naive hypotheses are aggregated up to create a more robust hypothesis. \n",
    "\n",
    "\n",
    "#### Strengths \n",
    "\n",
    "* Random forest is robust to outliers.\n",
    "* Give you a really good idea of which features in your data set are the most important.\n",
    "* Almost always have lower classification error and better f-scores than decision trees.\n",
    "* Deal really well with uneven data sets that have missing variables.\n",
    "* It can be used to generate very good classifiers if techniques (such as bagging) are used to “prune” the forest so it generalizes better. \n",
    "\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* This algorithm is computationally intensive and should use a relatively large amount of computing power.\n",
    "* For better accuracy, need more trees. This can slow down the training performance.\n",
    "* Will not work as well with a small dataset.\n",
    "* Prone to overfitting.\n",
    "\n",
    "\n",
    "#### Why did you choose this model to apply?\n",
    "\n",
    "* Although the dataset is small, I decided to try the Random forest model. I also wanted to see if a computationally intensive algorithm is viable on a small data set given the budget constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=60% id='DecisionTreeClassifier'><tr style='background-color:#E6E6E6; font-weight: bold;'><td width=25%>DecisionTreeClassifier</td><td colspan=3>Training set size </td></tr><tr style='background-color:#E6E6E6; font-weight: bold;'><td width=40%></td><td width=20%>100</td><td width=20%>200</td><td width=20%>300</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>Trainig Time</td><td width=20%>0.000</td><td width=20%>0.000</td><td width=20%>0.000</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>Prediction Time</td><td width=20%>0.000</td><td width=20%>0.000</td><td width=20%>0.000</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>F1 score for training set</td><td width=20%>1.000</td><td width=20%>1.000</td><td width=20%>1.000</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>F1 score for testing set</td><td width=20%>0.723</td><td width=20%>0.750</td><td width=20%>0.748</td></tr></table>"
      ],
      "text/plain": [
       "{'1_trainigTime': {'100': '0.000',\n",
       "  '200': '0.000',\n",
       "  '300': '0.000',\n",
       "  'name': 'Trainig Time',\n",
       "  'tableId': 'DecisionTreeClassifier'},\n",
       " '2_predictionTime': {'100': '0.000',\n",
       "  '200': '0.000',\n",
       "  '300': '0.000',\n",
       "  '95': '0.000',\n",
       "  'name': 'Prediction Time'},\n",
       " '3_train_f1_score': {'100': '1.000',\n",
       "  '200': '1.000',\n",
       "  '300': '1.000',\n",
       "  'name': 'F1 score for training set'},\n",
       " '4_test_f1_score': {'100': '0.723',\n",
       "  '200': '0.750',\n",
       "  '300': '0.748',\n",
       "  'name': 'F1 score for testing set'}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateTable(dic_DecisionTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=60% id='SVC'><tr style='background-color:#E6E6E6; font-weight: bold;'><td width=25%>SVC</td><td colspan=3>Training set size </td></tr><tr style='background-color:#E6E6E6; font-weight: bold;'><td width=40%></td><td width=20%>100</td><td width=20%>200</td><td width=20%>300</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>Trainig Time</td><td width=20%>0.004</td><td width=20%>0.004</td><td width=20%>0.012</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>Prediction Time</td><td width=20%>0.000</td><td width=20%>0.008</td><td width=20%>0.004</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>F1 score for training set</td><td width=20%>0.883</td><td width=20%>0.887</td><td width=20%>0.870</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>F1 score for testing set</td><td width=20%>0.792</td><td width=20%>0.789</td><td width=20%>0.803</td></tr></table>"
      ],
      "text/plain": [
       "{'1_trainigTime': {'100': '0.004',\n",
       "  '200': '0.004',\n",
       "  '300': '0.012',\n",
       "  'name': 'Trainig Time',\n",
       "  'tableId': 'SVC'},\n",
       " '2_predictionTime': {'100': '0.000',\n",
       "  '200': '0.008',\n",
       "  '300': '0.004',\n",
       "  '95': '0.000',\n",
       "  'name': 'Prediction Time'},\n",
       " '3_train_f1_score': {'100': '0.883',\n",
       "  '200': '0.887',\n",
       "  '300': '0.870',\n",
       "  'name': 'F1 score for training set'},\n",
       " '4_test_f1_score': {'100': '0.792',\n",
       "  '200': '0.789',\n",
       "  '300': '0.803',\n",
       "  'name': 'F1 score for testing set'}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateTable(dic_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=60% id='RandomForestClassifier'><tr style='background-color:#E6E6E6; font-weight: bold;'><td width=25%>RandomForestClassifier</td><td colspan=3>Training set size </td></tr><tr style='background-color:#E6E6E6; font-weight: bold;'><td width=40%></td><td width=20%>100</td><td width=20%>200</td><td width=20%>300</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>Trainig Time</td><td width=20%>0.028</td><td width=20%>0.043</td><td width=20%>0.047</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>Prediction Time</td><td width=20%>0.004</td><td width=20%>0.000</td><td width=20%>0.000</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>F1 score for training set</td><td width=20%>0.993</td><td width=20%>1.000</td><td width=20%>0.998</td></tr><tr><td width=40% style='background-color:#E6E6E6;'>F1 score for testing set</td><td width=20%>0.800</td><td width=20%>0.808</td><td width=20%>0.789</td></tr></table>"
      ],
      "text/plain": [
       "{'1_trainigTime': {'100': '0.028',\n",
       "  '200': '0.043',\n",
       "  '300': '0.047',\n",
       "  'name': 'Trainig Time',\n",
       "  'tableId': 'RandomForestClassifier'},\n",
       " '2_predictionTime': {'100': '0.004',\n",
       "  '200': '0.000',\n",
       "  '300': '0.000',\n",
       "  '95': '0.000',\n",
       "  'name': 'Prediction Time'},\n",
       " '3_train_f1_score': {'100': '0.993',\n",
       "  '200': '1.000',\n",
       "  '300': '0.998',\n",
       "  'name': 'F1 score for training set'},\n",
       " '4_test_f1_score': {'100': '0.800',\n",
       "  '200': '0.808',\n",
       "  '300': '0.789',\n",
       "  'name': 'F1 score for testing set'}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateTable(dic_RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "\n",
    "##### Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models performed well in case of time efficiency (training and predicting time). Only Random Forests were a bit slower while training the model comparatively to our other models. However the time difference is so insignificant that it cannot be the main factor to choose the best model for this problem. So the model in this case should be chosen based on the F1 scores that they produce.\n",
    "\n",
    "Even though Decision trees did perfect job on classifying training data correctly, where the F1 score for all training sizes is 1.0, they were not able to reproduce the same F1 score for the testing data that means that Decision Trees are performing poorly on unseen data. Also it seems that Random Forest Classifier got the best F1 score for training set at 300 training examples. In terms of the testing F1 scores it seems that SVM produced the best F1 score for 200 training examples. \n",
    "\n",
    "Considering all this I’ve choose SVM as the best model for this problem. SVM’s F1 score were consistent and changed very little with varying training dataset sizes. The F1 scores produced by SVM for testing data were quite good for all training sizes.\n",
    "\n",
    "Therefore, I chose SVM’s SVC as the best model to describe the data in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a type of linear separator. Suppose we want to split black circles from the white ones by drawing a line. Typically there are an infinite number of lines that will accomplish this task. SVMs, in particular, find the \"maximum-margin\" line - this is the line \"in the middle\". Intuitively, this works well because it allows for noise and is most tolerant to mistakes on either side. \n",
    "\n",
    "In case of a two dimensional problem, SVM will try to draw a curve between the different features to separate the outcomes. In our case we have multidimensional problem, so the SVM is going to try to make a surface, instead of a single curve, between all of those dimensions that best separates the students that graduated and those that did not. The best surface or curve is the one that maximizes the distance between the different points of that feature with the different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'kernel': ['linear', 'poly', 'rbf'], 'C': (0.05, 0.15, 0.25, 0.35, 0.45, 0.5), 'degree': [1, 2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring=make_scorer(f1_score, pos_label=yes), verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'kernel': ['linear', 'poly', 'rbf'], 'degree': [1,2,3,4,5], 'C': (0.05,0.15,0.25,0.35,0.45,0.50)}\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "clf = GridSearchCV(SVM_clf, parameters, scoring = f1_scorer)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### What is the model's final F<sub>1</sub> score?\n",
    "\n",
    "In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. F1 score is summarizing the number of correct positives and correct negatives out of all possible cases. \n",
    "\n",
    "The final F1 score that the tuned model can produce is 0.792, which is slightly better that what the default SVM could produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.819\n",
      "\n",
      "Best model parameter:  {'kernel': 'poly', 'C': 0.25, 'degree': 2}\n",
      "\n",
      "Best estimator:\n",
      "SVC(C=0.25, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma='auto', kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "best_F1_score = '{0:.3f}'.format(f1_score(clf.predict(X_test), y_test, pos_label='yes'))\n",
    "\n",
    "print \"Best F1 Score: \" +  best_F1_score\n",
    "print \"\\nBest model parameter:  \" + str( clf.best_params_)\n",
    "print \"\\nBest estimator:\\n{}\".format(clf.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
